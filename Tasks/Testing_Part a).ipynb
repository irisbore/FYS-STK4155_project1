{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 FYS-STK4155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a): Ordinary Least Square (OLS) on the Franke function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Franke Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from functions import FrankeFunction, create_design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data\n",
    "np.random.seed(2024)\n",
    "# Make data.\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "x, y = np.meshgrid(x,y)\n",
    "\n",
    "z = FrankeFunction(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#ax = fig.gca(projection='3d')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,\n",
    "linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-0.10, 1.40)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter(\"%.02f\"))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most off the code in the Franke function task is taken from the description in Project 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Code - check if it runs for two-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "#z = FrankeFunction(x, y)\n",
    "\n",
    "# Polynomial degree\n",
    "degree = 4\n",
    "\n",
    "# Creating design matrix\n",
    "X = create_design_matrix(x, degree = degree)\n",
    "\n",
    "#Adding a regresion term to avoid a singular matrix.\n",
    "reg_term = 1e-6\n",
    "OLSbeta = np.linalg.inv(X.T @ X + reg_term*np.eye(X.shape[1])) @ X.T @ y\n",
    "\n",
    "ytilde = X@OLSbeta\n",
    "#ztilde = X @ OLSbeta\n",
    "\n",
    "mse = np.mean((y - ytilde)**2)\n",
    "R2 = 1 - np.sum((y - ytilde)**2) / np.sum((y - np.mean(y))**2)\n",
    "\n",
    "print(f\"Beta: {OLSbeta}\")\n",
    "print(f\"Mse: {mse}\")\n",
    "print(f\"R2: {R2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Polynomial degrees\n",
    "degrees = np.arange(0, 5)\n",
    "\n",
    "# Empty lists to store scores and parameters\n",
    "beta_values = []\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "# Looping through each degree\n",
    "for degree in degrees:\n",
    "    # Creating design matrix\n",
    "    X = create_design_matrix(x, y, degree)\n",
    "    \n",
    "    # Calculating OLS beta\n",
    "    OLSbeta = np.linalg.inv(X.T @ X + reg_term*np.eye(X.shape[1])) @ X.T @ z\n",
    "    \n",
    "    # Calculating ztilde\n",
    "    ztilde = X @ OLSbeta\n",
    "    \n",
    "    # Calculating MSE and R2\n",
    "    mse = np.mean((z - ztilde)**2)\n",
    "    r2 = 1 - np.sum((z - ztilde)**2) / np.sum((z - np.mean(z))**2)\n",
    "    \n",
    "    # Appending beta values and scores\n",
    "    beta_values.append(OLSbeta)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MSE and R2 scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(degrees, mse_scores, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE as a function of Polynomial Degree')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(degrees, r2_scores, marker='o')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R2')\n",
    "plt.title('R2 as a function of Polynomial Degree')\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean squared error drops 0.375-0.025 = 0.35, which is 93.33 %, when the polynomial order is increased from 2 to 3. The R2 score indicated the exact same improvement as a little over 75% of the outcome is predicted by the model when the polynomial order is increased from 2 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Beta Values')\n",
    "plt.title('Beta Values as a function of Polynomial Degree')\n",
    "plt.grid()\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']  # Define colors for each degree\n",
    "\n",
    "for i in range(len(beta_values)):\n",
    "    for j in range(len(beta_values[i])):\n",
    "        beta_i = beta_values[i][j] \n",
    "        # Plot each beta value with a different color\n",
    "        plt.plot(degrees[i], beta_i, marker='o', color=colors[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', label=f'Degree = {degrees[i]}', markerfacecolor=colors[i], markersize=8) for i in range(len(degrees))])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit unsure how to plot the beta values as there are linearly increasing values with polynomial order. Note that where the number of beta values does not equal the number of polynomial order, there are beta values that are the same.\n",
    "\n",
    "We can see that the amount of unique betas increase with higher polynomial order. The unique betas seems to be more negative when the polynomial order is increased. The beta values are also more spread out when the polynomial order is increased for this particular dataset. This is reasonable as increasing the polynomial order will increase the complexity of the model, and the model will be able to fit the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Scaling and Centering of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(2024)\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "X = create_design_matrix(x, y, degree)\n",
    "\n",
    "# Split the data into training and test data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, z, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale and center the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copilotes thoughts on scaling and centering of the data: \n",
    "\n",
    "\"Scaling the data is important for several reasons:\n",
    "\n",
    "Normalization: Scaling the data brings all features to a similar scale, typically centered around zero. This is important because many machine learning algorithms assume that the features are normally distributed with zero mean and unit variance. If the features have different scales, it can lead to biased models and incorrect interpretations of feature importance.\n",
    "\n",
    "Outlier Sensitivity: Scaling helps in reducing the impact of outliers on the model. Outliers can have a disproportionate effect on the mean and variance of a feature, making it difficult for the model to learn patterns from the majority of the data. Scaling mitigates this issue by bringing the data within a similar range, making it less sensitive to outliers.\n",
    "\n",
    "Convergence Speed: Scaling can improve the convergence speed of optimization algorithms. When features have different scales, the optimization process may take longer to find the optimal solution. Scaling the data helps in achieving faster convergence and more efficient training.\n",
    "\n",
    "However, there are cases where scaling may not be necessary or even detrimental:\n",
    "\n",
    "Domain Knowledge: In some cases, domain knowledge may suggest that scaling is not required. For example, if the features already have a similar scale and there is no reason to believe that scaling would improve the model's performance, then scaling may be unnecessary.\n",
    "\n",
    "Sparse Data: Scaling sparse data can be challenging because it may break the sparsity structure and result in a dense matrix. In the provided code, there is a check to avoid centering sparse matrices. This is because centering sparse matrices would require building a dense matrix, which may not be feasible for large datasets. In such cases, it is recommended to pass with_mean=False to the StandardScaler constructor to avoid centering.\n",
    "\n",
    "Interpretability: Scaling can sometimes make it harder to interpret the model's coefficients or feature importance. If interpretability is a priority and the scale of the features is meaningful in the context of the problem, then scaling may not be desirable.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
