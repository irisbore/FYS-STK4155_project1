{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d): Paper and pencil part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation value y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of ordinary least squares (OLS), the vector of observations $( \\mathbf{y} )$ is modeled as:\n",
    "\n",
    "$\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "\n",
    "For each element $( y_i )$ in the vector $( \\mathbf{y} )$, we have:\n",
    "\n",
    "$y_i = \\sum_j x_{ij} \\beta_j + \\epsilon_i$\n",
    "\n",
    "where $( x_{ij} )$ is the element in the $( i )-th$ row and $( j )-th$ column of the matrix $( \\mathbf{X} )$, and $( \\epsilon_i )$ is the corresponding error for the $( i )-th$ observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}(y_i) = \\mathbb{E}\\left( \\sum_j x_{ij} \\beta_j + \\epsilon_i \\right)$\n",
    "\n",
    "Using the linearity of expectation:\n",
    "\n",
    "$\\mathbb{E}(y_i) = \\sum_j x_{ij} \\beta_j + \\mathbb{E}(\\epsilon_i)$\n",
    "\n",
    "Since $( \\epsilon_i )$ is normally distributed with mean 0, we have:\n",
    "\n",
    "$\\mathbb{E}(\\epsilon_i) = 0$\n",
    "\n",
    "$\\mathbb{E}(y_i) = \\sum_j x_{ij} \\beta_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In matrix form, this can be written as:\n",
    "\n",
    "$\\mathbb{E}(y_i) = \\mathbf{X}_{i,*} \\boldsymbol{\\beta}$\n",
    "\n",
    "where $( \\mathbf{X}_{i,*} )$ is the $( i )-th$ row of the design matrix $( \\mathbf{X} )$\n",
    "\n",
    "We have now shown that the expectation value of $( y_i )$ is:\n",
    "\n",
    "$\\mathbb{E}(y_i) = \\sum_j x_{ij} \\beta_j = \\mathbf{X}_{i,*} \\boldsymbol{\\beta}$\n",
    "\n",
    "This is what we where supposed to show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are supposed to find the variance: \n",
    "\n",
    "$\\text{Var}(y_i) = \\text{Var}\\left(\\sum_j x_{ij} \\beta_j + \\epsilon_i\\right)$\n",
    "\n",
    "Since the deterministic part $( \\sum_j x_{ij} \\beta_j )$ is constant with respect to the random error $( \\epsilon_i )$, its variance is 0. Thus, we have:\n",
    "\n",
    "$\\text{Var}(y_i) = \\text{Var}(\\epsilon_i)$\n",
    "\n",
    "From the assumptions, we know that $(\\epsilon_i \\sim N(0, \\sigma^2))$, so:\n",
    "\n",
    "$\\text{Var}(\\epsilon_i) = \\sigma^2$\n",
    "\n",
    "Thus, we can then conclude that:\n",
    "\n",
    "$\\text{Var}(y_i) = \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation value Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS estimator for the regression coefficients $( \\hat{\\beta} )$ is given by the well-known formula:\n",
    "\n",
    "$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$\n",
    "\n",
    "\n",
    "By substituting $( \\mathbf{y} = \\mathbf{X} \\beta + \\boldsymbol{\\epsilon} )$ into the expression for $( \\hat{\\beta} )$ we get:\n",
    "\n",
    "$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{X} \\beta + \\boldsymbol{\\epsilon})$\n",
    "\n",
    "\n",
    "Using the distributive property of matrix multiplication:\n",
    "\n",
    "$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{X} \\beta + (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}$\n",
    "\n",
    "The first term simplifies because $( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{X} = \\mathbf{I} )$:\n",
    "\n",
    "$\\hat{\\beta} = \\beta + (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}$\n",
    "\n",
    "\n",
    "Now, we take the expectation of both sides:\n",
    "\n",
    "$\\mathbb{E}[\\hat{\\beta}] = \\mathbb{E}[\\beta + (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}]$\n",
    "\n",
    "Since $( \\beta )$ is a constant and the expectation operator is linear:\n",
    "\n",
    "$\\mathbb{E}[\\hat{\\beta}] = \\beta + \\mathbb{E}[(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}]$\n",
    "\n",
    "The term $( (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon} )$ involves the error $( \\boldsymbol{\\epsilon} )$, and since $( \\mathbb{E}[\\boldsymbol{\\epsilon}] = 0 )$, we have:\n",
    "\n",
    "$\\mathbb{E}[(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}] = 0$\n",
    "\n",
    "\n",
    "Which means that: \n",
    "\n",
    "$\\mathbb{E}[\\hat{\\beta}] = \\beta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the expression for the OLS estimator:\n",
    "\n",
    "$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$\n",
    "\n",
    "Using the model $( \\mathbf{y} = \\mathbf{X} \\beta + \\boldsymbol{\\epsilon} )$, we substitute this into the expression for $( \\hat{\\beta} )$:\n",
    "\n",
    "$\\hat{\\beta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top (\\mathbf{X} \\beta + \\boldsymbol{\\epsilon})$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$\\hat{\\beta} = \\beta + (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}$\n",
    "\n",
    "We now want to compute the variance of $( \\hat{\\beta} )$. The variance operator acts only on the random error term $( \\boldsymbol{\\epsilon} )$, since $( \\beta )$ is deterministic.\n",
    "\n",
    "Thus, the variance of $( \\hat{\\beta} )$ is:\n",
    "\n",
    "$\\text{Var}(\\hat{\\beta}) = \\text{Var}\\left((\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\epsilon}\\right)$\n",
    "\n",
    "Using the fact that for a random vector $( \\mathbf{A} \\mathbf{Z} ),$ where $( \\mathbf{Z} )$ is a random vector and $( \\mathbf{A} )$ is a matrix, the variance is:\n",
    "\n",
    "$\\text{Var}(\\mathbf{A} \\mathbf{Z}) = \\mathbf{A} \\, \\text{Var}(\\mathbf{Z}) \\, \\mathbf{A}^\\top$\n",
    "\n",
    "Here, $( \\mathbf{A} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top )$ and $( \\boldsymbol{\\epsilon} \\sim N(0, \\sigma^2 \\mathbf{I}))$. The variance of $( \\boldsymbol{\\epsilon} )$ is:\n",
    "\n",
    "$\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2 \\mathbf{I}$\n",
    "\n",
    "Which means that the variance of $( \\hat{\\beta} )$ becomes:\n",
    "\n",
    "$\\text{Var}(\\hat{\\beta}) = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\, \\sigma^2 \\mathbf{I} \\, \\mathbf{X} \\, (\\mathbf{X}^\\top \\mathbf{X})^{-1}$\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "$\\text{Var}(\\hat{\\beta}) = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
