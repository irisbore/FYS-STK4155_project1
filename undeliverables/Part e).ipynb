{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part e): Bias-variance trade-off and resampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduction of Bias-variance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for model complexity and errors\n",
    "model_complexity = np.linspace(0, 10, 100)\n",
    "\n",
    "# Generate training and test error data\n",
    "training_error = np.exp(-0.3 * model_complexity) + 0.1 * np.random.rand(100)\n",
    "test_error = np.exp(-0.3 * model_complexity) + 0.1 * model_complexity + 0.2 * np.random.rand(100)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(model_complexity, training_error, label=\"Training Sample\", color='teal', linewidth=2)\n",
    "plt.plot(model_complexity, test_error, label=\"Test Sample\", color='red', linewidth=2)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Model Complexity\")\n",
    "plt.ylabel(\"Prediction Error\")\n",
    "plt.title(\"Test and Training Error as a function of Model Complexity\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting the cost-function in terms of bias, variance and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to show that this expression can be written as:\n",
    "\n",
    "$\\mathbb{E}[(y - \\tilde{y})^2] = \\text{Bias}[\\tilde{y}] + \\text{Var}[\\tilde{y}] + \\sigma^2$\n",
    "\n",
    "We decompose the expected value $( \\mathbb{E}[(y - \\tilde{y})^2] )$ into bias, variance, and noise. To do this, letâ€™s add and subtract $( \\mathbb{E}[\\tilde{y}] )$, the expected value of the prediction:\n",
    "\n",
    "$y - \\tilde{y} = \\left( y - \\mathbb{E}[\\tilde{y}] \\right) + \\left( \\mathbb{E}[\\tilde{y}] - \\tilde{y} \\right)$\n",
    "\n",
    "Now, we square both sides:\n",
    "\n",
    "$(y - \\tilde{y})^2 = \\left( y - \\mathbb{E}[\\tilde{y}] \\right)^2 + \\left( \\mathbb{E}[\\tilde{y}] - \\tilde{y} \\right)^2 + 2\\left( y - \\mathbb{E}[\\tilde{y}] \\right)\\left( \\mathbb{E}[\\tilde{y}] - \\tilde{y} \\right)$\n",
    "\n",
    "Now, take the expectation $( \\mathbb{E} )$ of both sides. First, note that $( \\mathbb{E}[\\tilde{y}] )$ is a constant, so:\n",
    "\n",
    "$\\mathbb{E}[(y - \\tilde{y})^2] = \\mathbb{E} \\left[ (y - \\mathbb{E}[\\tilde{y}])^2 \\right] + \\mathbb{E} \\left[ (\\tilde{y} - \\mathbb{E}[\\tilde{y}])^2 \\right] + 2 \\mathbb{E} \\left[ (y - \\mathbb{E}[\\tilde{y}]) (\\mathbb{E}[\\tilde{y}] - \\tilde{y}) \\right]$\n",
    "\n",
    "The first term $( \\mathbb{E}[(y - \\mathbb{E}[\\tilde{y}])^2] )$ is the variance of the true data around the expected prediction, which includes the bias term:\n",
    "\n",
    "$\\mathbb{E}[(y - \\mathbb{E}[\\tilde{y}])^2] = \\text{Bias}[\\tilde{y}]^2 + \\sigma^2$\n",
    "\n",
    "\n",
    "\n",
    "The second term $( \\mathbb{E}[(\\tilde{y} - \\mathbb{E}[\\tilde{y}])^2] )$ is the variance of the model:\n",
    "\n",
    "$\\mathbb{E}[(\\tilde{y} - \\mathbb{E}[\\tilde{y}])^2] = \\text{Var}[\\tilde{y}]$\n",
    "\n",
    "\n",
    "The cross-term expectation $( \\mathbb{E}[(y - \\mathbb{E}[\\tilde{y}])(\\mathbb{E}[\\tilde{y}] - \\tilde{y})] )$ is zero because $( y - \\mathbb{E}[\\tilde{y}] )$ and $( \\tilde{y} - \\mathbb{E}[\\tilde{y}] $) are independent.\n",
    "\n",
    "\n",
    "Thus, we can simplify the decomposition to:\n",
    "\n",
    "$\\mathbb{E}[(y - \\tilde{y})^2] = \\text{Bias}[\\tilde{y}]^2 + \\text{Var}[\\tilde{y}] + \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance analysis of the Franke function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the bias-variance trade-off, we can use the bootstrap resampling method. This method involves randomly sampling the training data with replacement to create multiple bootstrap samples. Each bootstrap sample is used to train a model, and the predictions from these models are averaged to obtain the final prediction. By repeating this process multiple times, we can estimate the bias and variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "\n",
    "n = 100\n",
    "n_boostraps = 100\n",
    "degree = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "noise = 1\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(-1, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)\n",
    "\n",
    "# Splitting the data into training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "error = np.zeros(len(degree))\n",
    "bias = np.zeros(len(degree))\n",
    "variance = np.zeros(len(degree))\n",
    "\n",
    "for i in degree:\n",
    "    # Combine x transformation and model into one operation.\n",
    "    model = make_pipeline(PolynomialFeatures(degree=i), LinearRegression(fit_intercept=False))\n",
    "\n",
    "    # The following (m x n_bootstraps) matrix holds the column vectors y_pred\n",
    "    # for each bootstrap iteration.\n",
    "    y_pred = np.empty((y_test.shape[0], n_boostraps))\n",
    "    for j in range(n_boostraps):\n",
    "        x_, y_ = resample(x_train, y_train)\n",
    "\n",
    "        # Evaluate the new model on the same test data each time.\n",
    "        y_pred[:, j] = model.fit(x_, y_).predict(x_test).ravel()\n",
    "\n",
    "        # Note: Expectations and variances taken w.r.t. different training\n",
    "        # data sets, hence the axis=1. Subsequent means are taken across the test data\n",
    "        # set in order to obtain a total value, but before this we have error/bias/variance\n",
    "        # calculated per data point in the test set.\n",
    "        # Note 2: The use of keepdims=True is important in the calculation of bias as this \n",
    "        # maintains the column vector form. Dropping this yields very unexpected results.\n",
    "    error[i] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True) )\n",
    "    bias[i] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )\n",
    "    variance[i] = np.mean( np.var(y_pred, axis=1, keepdims=True) )\n",
    "\n",
    "\n",
    "plt.title('Bias variance tradeoff')\n",
    "plt.xlabel('Model complexity (degree)')\n",
    "plt.ylabel('Error')\n",
    "plt.plot(degree, error, label='Error')\n",
    "plt.plot(degree, bias, label='Bias')\n",
    "plt.plot(degree, variance, label='Variance')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Discuss the bias and variance trade-off as function of your model complexity\n",
    "(the degree of the polynomial) and the number of data points, and possibly also\n",
    "your training and test data using the bootstrap resampling method.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
